{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCL98_i2BLlX",
        "colab_type": "code",
        "outputId": "00c5ec18-ac3a-4963-b744-42010f1b07dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/78/f5e285f6923ed8fdf8ead8faccd1c7ea34bd1500d1668d615794285db3b4/pybullet-2.6.6-cp36-cp36m-manylinux1_x86_64.whl (94.2MB)\n",
            "\u001b[K     |████████████████████████████████| 94.2MB 109kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pybullet) (1.17.5)\n",
            "Installing collected packages: pybullet\n",
            "Successfully installed pybullet-2.6.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iD8nepvBlYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRaMO_tFCOt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else: self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind:\n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxtq8jHnQP8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_actions):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_actions = max_actions\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_actions * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuJtVavnTlEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x = F.relu(self.layer_1(xu))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.layer_3(x)\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "espgvc-0bNHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TD3(object):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iteration, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iteration):\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "      next_action = self.actor_target(next_state)\n",
        "\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      target_Q = reward + ((1-done) * discount * target_Q).detach()\n",
        "\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IakCEMBfiWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward=0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print(f'Average Reward for the Evaluation step : {avg_reward}')\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca161Agc6Hyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"HalfCheetahBulletEnv-v0\"\n",
        "# env_name = \"Walker2DBulletEnv-v0\"\n",
        "# env_name = \"AntBulletEnv-v0\"\n",
        "seed = 0\n",
        "start_timesteps = 1e4\n",
        "eval_freq = 5e3\n",
        "max_timesteps = 5e5\n",
        "save_model = True\n",
        "expl_noise = 0.1\n",
        "batch_size = 100\n",
        "discount = 0.99\n",
        "tau = 0.005\n",
        "policy_noise = 0.2\n",
        "noise_clip = 0.5\n",
        "policy_freq = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4NoSTri7KFl",
        "colab_type": "code",
        "outputId": "d1f99131-cccb-426d-a6a5-6017ea1cbb6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "file_name = '%s_%s_%s' % ('TD3', env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print(f'Setting : {file_name}')\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Setting : TD3_HalfCheetahBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTWMMJ437jdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('./results'):\n",
        "  os.makedirs('./results')\n",
        "if save_model and not os.path.exists('./pytorch_models'):\n",
        "  os.makedirs('./pytorch_models')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59VZR2Lu8ECB",
        "colab_type": "code",
        "outputId": "e0c950ad-a64f-47c3-8203-8060745b2368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kfq556I8K1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcSmEsC69MYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-auzg4Y9Nle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lrzcb_j98RJ",
        "colab_type": "code",
        "outputId": "e87c00d7-c59f-404f-d76a-15426340b22a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : -1430.0186516188799\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ-tMwKjA--0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "  path = os.path.join(base, name)\n",
        "  if not os.path.exists(path): os.makedirs(path)\n",
        "  return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force=True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5e1UaXjE3D3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2D2JKRzF9Ev",
        "colab_type": "code",
        "outputId": "dd7814ad-43dc-4f4c-eb01-8c6e1e79f612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "while total_timesteps < max_timesteps:\n",
        "\n",
        "  if done:\n",
        "\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    if timesteps_since_eval > eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, './pytorch_models')\n",
        "      np.save('./results/%s' % (file_name), evaluations)\n",
        "\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else:\n",
        "    action = policy.select_action(obs)\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "    \n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "\n",
        "  episode_reward += reward\n",
        "\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_model: policy.save(file_name, './pytorch_models')\n",
        "np.save('./results/%s' % (file_name), evaluations)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: -1363.5071381699986\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: -1250.27706871196\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: -1351.294472090137\n",
            "Total Timesteps: 4000 Episode Num: 4 Reward: -1389.4235294924033\n",
            "Total Timesteps: 5000 Episode Num: 5 Reward: -1420.837576647368\n",
            "Total Timesteps: 6000 Episode Num: 6 Reward: -1320.7896599930443\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : -1517.2109985635575\n",
            "---------------------------------------\n",
            "Total Timesteps: 7000 Episode Num: 7 Reward: -1346.625283981632\n",
            "Total Timesteps: 8000 Episode Num: 8 Reward: -1174.5423747559937\n",
            "Total Timesteps: 9000 Episode Num: 9 Reward: -1325.7769359241984\n",
            "Total Timesteps: 10000 Episode Num: 10 Reward: -1318.6172971308604\n",
            "Total Timesteps: 11000 Episode Num: 11 Reward: -1631.840427992441\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : -1457.5184186917638\n",
            "---------------------------------------\n",
            "Total Timesteps: 12000 Episode Num: 12 Reward: -1465.3427370047268\n",
            "Total Timesteps: 13000 Episode Num: 13 Reward: 157.76673915211418\n",
            "Total Timesteps: 14000 Episode Num: 14 Reward: -1410.9145567117232\n",
            "Total Timesteps: 15000 Episode Num: 15 Reward: -1674.479663865578\n",
            "Total Timesteps: 16000 Episode Num: 16 Reward: 390.50708897398414\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : -1054.0416561274042\n",
            "---------------------------------------\n",
            "Total Timesteps: 17000 Episode Num: 17 Reward: -663.9507488849036\n",
            "Total Timesteps: 18000 Episode Num: 18 Reward: -1354.0852311572573\n",
            "Total Timesteps: 19000 Episode Num: 19 Reward: -1009.6626655675079\n",
            "Total Timesteps: 20000 Episode Num: 20 Reward: -988.9749279776225\n",
            "Total Timesteps: 21000 Episode Num: 21 Reward: 413.8167680788841\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 111.69455375427574\n",
            "---------------------------------------\n",
            "Total Timesteps: 22000 Episode Num: 22 Reward: -180.72330414324003\n",
            "Total Timesteps: 23000 Episode Num: 23 Reward: -1219.3854707068738\n",
            "Total Timesteps: 24000 Episode Num: 24 Reward: -1288.2913080076708\n",
            "Total Timesteps: 25000 Episode Num: 25 Reward: -1313.5936979610528\n",
            "Total Timesteps: 26000 Episode Num: 26 Reward: -1009.3211307800755\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 43.71316396130695\n",
            "---------------------------------------\n",
            "Total Timesteps: 27000 Episode Num: 27 Reward: 437.6186895478066\n",
            "Total Timesteps: 28000 Episode Num: 28 Reward: 331.3883933973537\n",
            "Total Timesteps: 29000 Episode Num: 29 Reward: -1533.6312674811024\n",
            "Total Timesteps: 30000 Episode Num: 30 Reward: 493.04304753466164\n",
            "Total Timesteps: 31000 Episode Num: 31 Reward: -213.48911872265427\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 273.9947666840628\n",
            "---------------------------------------\n",
            "Total Timesteps: 32000 Episode Num: 32 Reward: -60.88397299479221\n",
            "Total Timesteps: 33000 Episode Num: 33 Reward: 534.8051753311765\n",
            "Total Timesteps: 34000 Episode Num: 34 Reward: -1282.8082751455102\n",
            "Total Timesteps: 35000 Episode Num: 35 Reward: -189.53267001502624\n",
            "Total Timesteps: 36000 Episode Num: 36 Reward: 159.65801113810454\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 444.2641916416469\n",
            "---------------------------------------\n",
            "Total Timesteps: 37000 Episode Num: 37 Reward: 368.3702541038879\n",
            "Total Timesteps: 38000 Episode Num: 38 Reward: 29.99572025645731\n",
            "Total Timesteps: 39000 Episode Num: 39 Reward: 332.88987104422955\n",
            "Total Timesteps: 40000 Episode Num: 40 Reward: -65.57843743470539\n",
            "Total Timesteps: 41000 Episode Num: 41 Reward: -44.16420516166019\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 114.99547432592166\n",
            "---------------------------------------\n",
            "Total Timesteps: 42000 Episode Num: 42 Reward: 14.080972995917113\n",
            "Total Timesteps: 43000 Episode Num: 43 Reward: 185.6435693486865\n",
            "Total Timesteps: 44000 Episode Num: 44 Reward: 72.63721239131716\n",
            "Total Timesteps: 45000 Episode Num: 45 Reward: -73.95073116033252\n",
            "Total Timesteps: 46000 Episode Num: 46 Reward: 321.80895986923804\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 456.278675157263\n",
            "---------------------------------------\n",
            "Total Timesteps: 47000 Episode Num: 47 Reward: 570.183871271275\n",
            "Total Timesteps: 48000 Episode Num: 48 Reward: 436.01710864838253\n",
            "Total Timesteps: 49000 Episode Num: 49 Reward: 528.1196266988086\n",
            "Total Timesteps: 50000 Episode Num: 50 Reward: 561.0749965119275\n",
            "Total Timesteps: 51000 Episode Num: 51 Reward: 603.8850945521594\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 515.8515223083494\n",
            "---------------------------------------\n",
            "Total Timesteps: 52000 Episode Num: 52 Reward: 572.33262541855\n",
            "Total Timesteps: 53000 Episode Num: 53 Reward: 617.624085885535\n",
            "Total Timesteps: 54000 Episode Num: 54 Reward: 639.1846845543972\n",
            "Total Timesteps: 55000 Episode Num: 55 Reward: 479.9319463490519\n",
            "Total Timesteps: 56000 Episode Num: 56 Reward: 456.8642894519491\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 608.2559112470544\n",
            "---------------------------------------\n",
            "Total Timesteps: 57000 Episode Num: 57 Reward: 457.1292679164417\n",
            "Total Timesteps: 58000 Episode Num: 58 Reward: 579.5118921309196\n",
            "Total Timesteps: 59000 Episode Num: 59 Reward: 522.6574276563554\n",
            "Total Timesteps: 60000 Episode Num: 60 Reward: 569.5745456950337\n",
            "Total Timesteps: 61000 Episode Num: 61 Reward: 585.0353727972699\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 526.9211160502681\n",
            "---------------------------------------\n",
            "Total Timesteps: 62000 Episode Num: 62 Reward: 624.9510553593904\n",
            "Total Timesteps: 63000 Episode Num: 63 Reward: 645.4005193805538\n",
            "Total Timesteps: 64000 Episode Num: 64 Reward: 522.2123369544711\n",
            "Total Timesteps: 65000 Episode Num: 65 Reward: 433.47244847996495\n",
            "Total Timesteps: 66000 Episode Num: 66 Reward: -1637.105145756304\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : -1583.436823285262\n",
            "---------------------------------------\n",
            "Total Timesteps: 67000 Episode Num: 67 Reward: -1599.707137887812\n",
            "Total Timesteps: 68000 Episode Num: 68 Reward: -1737.6573324759424\n",
            "Total Timesteps: 69000 Episode Num: 69 Reward: -1730.8894931646196\n",
            "Total Timesteps: 70000 Episode Num: 70 Reward: -1697.7042284051993\n",
            "Total Timesteps: 71000 Episode Num: 71 Reward: -1617.919685898966\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : -1633.6602636665846\n",
            "---------------------------------------\n",
            "Total Timesteps: 72000 Episode Num: 72 Reward: -1634.3492425133945\n",
            "Total Timesteps: 73000 Episode Num: 73 Reward: -1633.8598712238454\n",
            "Total Timesteps: 74000 Episode Num: 74 Reward: -1542.7312854189388\n",
            "Total Timesteps: 75000 Episode Num: 75 Reward: -1669.2103639360166\n",
            "Total Timesteps: 76000 Episode Num: 76 Reward: -1628.6906612572423\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : -1486.8140391294555\n",
            "---------------------------------------\n",
            "Total Timesteps: 77000 Episode Num: 77 Reward: -1637.9025836327523\n",
            "Total Timesteps: 78000 Episode Num: 78 Reward: -392.9624448580223\n",
            "Total Timesteps: 79000 Episode Num: 79 Reward: -128.15397071656434\n",
            "Total Timesteps: 80000 Episode Num: 80 Reward: -88.37471908389293\n",
            "Total Timesteps: 81000 Episode Num: 81 Reward: -1526.181016664547\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 167.6758432480521\n",
            "---------------------------------------\n",
            "Total Timesteps: 82000 Episode Num: 82 Reward: 350.72397661622455\n",
            "Total Timesteps: 83000 Episode Num: 83 Reward: 314.17881534228735\n",
            "Total Timesteps: 84000 Episode Num: 84 Reward: 411.6325044034694\n",
            "Total Timesteps: 85000 Episode Num: 85 Reward: 604.0788164439269\n",
            "Total Timesteps: 86000 Episode Num: 86 Reward: 522.8781022757122\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 106.66466805378975\n",
            "---------------------------------------\n",
            "Total Timesteps: 87000 Episode Num: 87 Reward: 440.4357470636088\n",
            "Total Timesteps: 88000 Episode Num: 88 Reward: 534.7262795235324\n",
            "Total Timesteps: 89000 Episode Num: 89 Reward: 532.5112711688214\n",
            "Total Timesteps: 90000 Episode Num: 90 Reward: 378.0608673073286\n",
            "Total Timesteps: 91000 Episode Num: 91 Reward: 535.9300190834193\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 528.1525041271522\n",
            "---------------------------------------\n",
            "Total Timesteps: 92000 Episode Num: 92 Reward: 450.64350339462305\n",
            "Total Timesteps: 93000 Episode Num: 93 Reward: 538.4947347457187\n",
            "Total Timesteps: 94000 Episode Num: 94 Reward: 544.7254189250355\n",
            "Total Timesteps: 95000 Episode Num: 95 Reward: 666.7831716041429\n",
            "Total Timesteps: 96000 Episode Num: 96 Reward: 557.7997182215794\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 642.267849579971\n",
            "---------------------------------------\n",
            "Total Timesteps: 97000 Episode Num: 97 Reward: 654.37962854819\n",
            "Total Timesteps: 98000 Episode Num: 98 Reward: 681.2860825045221\n",
            "Total Timesteps: 99000 Episode Num: 99 Reward: 497.36787937217065\n",
            "Total Timesteps: 100000 Episode Num: 100 Reward: 563.2796187355908\n",
            "Total Timesteps: 101000 Episode Num: 101 Reward: 691.5773271780395\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 549.6473284863596\n",
            "---------------------------------------\n",
            "Total Timesteps: 102000 Episode Num: 102 Reward: 550.6977507341653\n",
            "Total Timesteps: 103000 Episode Num: 103 Reward: 569.6703509464865\n",
            "Total Timesteps: 104000 Episode Num: 104 Reward: 505.58516174435823\n",
            "Total Timesteps: 105000 Episode Num: 105 Reward: 461.2874630899305\n",
            "Total Timesteps: 106000 Episode Num: 106 Reward: 565.0316241940161\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 440.3995273714783\n",
            "---------------------------------------\n",
            "Total Timesteps: 107000 Episode Num: 107 Reward: 447.71239270136283\n",
            "Total Timesteps: 108000 Episode Num: 108 Reward: 470.3584933103904\n",
            "Total Timesteps: 109000 Episode Num: 109 Reward: 543.7835663325643\n",
            "Total Timesteps: 110000 Episode Num: 110 Reward: 475.8803456174876\n",
            "Total Timesteps: 111000 Episode Num: 111 Reward: 353.2506216917244\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 425.390729076452\n",
            "---------------------------------------\n",
            "Total Timesteps: 112000 Episode Num: 112 Reward: 407.5277861183733\n",
            "Total Timesteps: 113000 Episode Num: 113 Reward: 433.46226761983837\n",
            "Total Timesteps: 114000 Episode Num: 114 Reward: 490.8474123805012\n",
            "Total Timesteps: 115000 Episode Num: 115 Reward: 458.67825386144807\n",
            "Total Timesteps: 116000 Episode Num: 116 Reward: 575.8691517162705\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 503.38894947106036\n",
            "---------------------------------------\n",
            "Total Timesteps: 117000 Episode Num: 117 Reward: 527.1269087168621\n",
            "Total Timesteps: 118000 Episode Num: 118 Reward: 639.4504962450792\n",
            "Total Timesteps: 119000 Episode Num: 119 Reward: 597.6316953688204\n",
            "Total Timesteps: 120000 Episode Num: 120 Reward: 555.9873934579855\n",
            "Total Timesteps: 121000 Episode Num: 121 Reward: 560.682132460279\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 586.7688357443276\n",
            "---------------------------------------\n",
            "Total Timesteps: 122000 Episode Num: 122 Reward: 566.4011356665819\n",
            "Total Timesteps: 123000 Episode Num: 123 Reward: 553.4347341583017\n",
            "Total Timesteps: 124000 Episode Num: 124 Reward: 553.9373247083203\n",
            "Total Timesteps: 125000 Episode Num: 125 Reward: 567.3057274235672\n",
            "Total Timesteps: 126000 Episode Num: 126 Reward: 598.3634445759287\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 553.2215580794049\n",
            "---------------------------------------\n",
            "Total Timesteps: 127000 Episode Num: 127 Reward: 550.2726911160993\n",
            "Total Timesteps: 128000 Episode Num: 128 Reward: -1251.8226630758109\n",
            "Total Timesteps: 129000 Episode Num: 129 Reward: 243.16669761859222\n",
            "Total Timesteps: 130000 Episode Num: 130 Reward: 608.9274805241131\n",
            "Total Timesteps: 131000 Episode Num: 131 Reward: 613.3204604282565\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 605.0069581619257\n",
            "---------------------------------------\n",
            "Total Timesteps: 132000 Episode Num: 132 Reward: 560.0843774328117\n",
            "Total Timesteps: 133000 Episode Num: 133 Reward: 568.7549297209794\n",
            "Total Timesteps: 134000 Episode Num: 134 Reward: 470.0513952795984\n",
            "Total Timesteps: 135000 Episode Num: 135 Reward: 579.0256445347293\n",
            "Total Timesteps: 136000 Episode Num: 136 Reward: 680.7335850382676\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 628.3731252972354\n",
            "---------------------------------------\n",
            "Total Timesteps: 137000 Episode Num: 137 Reward: 633.1890540537433\n",
            "Total Timesteps: 138000 Episode Num: 138 Reward: 532.8908735495797\n",
            "Total Timesteps: 139000 Episode Num: 139 Reward: 541.1956200881724\n",
            "Total Timesteps: 140000 Episode Num: 140 Reward: 514.856111976833\n",
            "Total Timesteps: 141000 Episode Num: 141 Reward: 647.3946441786602\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 637.0461952057121\n",
            "---------------------------------------\n",
            "Total Timesteps: 142000 Episode Num: 142 Reward: 631.8798223486931\n",
            "Total Timesteps: 143000 Episode Num: 143 Reward: 517.7133432856795\n",
            "Total Timesteps: 144000 Episode Num: 144 Reward: 652.045432940363\n",
            "Total Timesteps: 145000 Episode Num: 145 Reward: 621.4221381536579\n",
            "Total Timesteps: 146000 Episode Num: 146 Reward: 542.7711028981778\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 659.1042262687654\n",
            "---------------------------------------\n",
            "Total Timesteps: 147000 Episode Num: 147 Reward: 666.4185322876343\n",
            "Total Timesteps: 148000 Episode Num: 148 Reward: 676.1899316028594\n",
            "Total Timesteps: 149000 Episode Num: 149 Reward: 697.3571169157294\n",
            "Total Timesteps: 150000 Episode Num: 150 Reward: 674.8057723542075\n",
            "Total Timesteps: 151000 Episode Num: 151 Reward: 662.2926994592126\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 680.3300681029422\n",
            "---------------------------------------\n",
            "Total Timesteps: 152000 Episode Num: 152 Reward: 687.5023386151887\n",
            "Total Timesteps: 153000 Episode Num: 153 Reward: 624.7555759013688\n",
            "Total Timesteps: 154000 Episode Num: 154 Reward: 556.016940473378\n",
            "Total Timesteps: 155000 Episode Num: 155 Reward: 631.365948658843\n",
            "Total Timesteps: 156000 Episode Num: 156 Reward: 688.1565888714528\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 612.7979285000964\n",
            "---------------------------------------\n",
            "Total Timesteps: 157000 Episode Num: 157 Reward: 640.5600841673845\n",
            "Total Timesteps: 158000 Episode Num: 158 Reward: 688.5764917385022\n",
            "Total Timesteps: 159000 Episode Num: 159 Reward: 598.9804473577586\n",
            "Total Timesteps: 160000 Episode Num: 160 Reward: 645.2594754486496\n",
            "Total Timesteps: 161000 Episode Num: 161 Reward: 615.8449316045354\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 592.4927965680756\n",
            "---------------------------------------\n",
            "Total Timesteps: 162000 Episode Num: 162 Reward: 665.3387804939805\n",
            "Total Timesteps: 163000 Episode Num: 163 Reward: 647.9382509540488\n",
            "Total Timesteps: 164000 Episode Num: 164 Reward: 579.3381846227683\n",
            "Total Timesteps: 165000 Episode Num: 165 Reward: 646.3545592621546\n",
            "Total Timesteps: 166000 Episode Num: 166 Reward: 667.0303372942524\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 629.1187912690136\n",
            "---------------------------------------\n",
            "Total Timesteps: 167000 Episode Num: 167 Reward: 579.7718441252398\n",
            "Total Timesteps: 168000 Episode Num: 168 Reward: 677.9296372059605\n",
            "Total Timesteps: 169000 Episode Num: 169 Reward: 560.5664108619934\n",
            "Total Timesteps: 170000 Episode Num: 170 Reward: 603.9136431379986\n",
            "Total Timesteps: 171000 Episode Num: 171 Reward: 560.8317062579177\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 572.3840245118489\n",
            "---------------------------------------\n",
            "Total Timesteps: 172000 Episode Num: 172 Reward: 578.2702457798569\n",
            "Total Timesteps: 173000 Episode Num: 173 Reward: 577.1802539719966\n",
            "Total Timesteps: 174000 Episode Num: 174 Reward: 564.7107891061296\n",
            "Total Timesteps: 175000 Episode Num: 175 Reward: 582.7646569023747\n",
            "Total Timesteps: 176000 Episode Num: 176 Reward: 591.8491831987106\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 581.5816998830886\n",
            "---------------------------------------\n",
            "Total Timesteps: 177000 Episode Num: 177 Reward: 590.8404588225416\n",
            "Total Timesteps: 178000 Episode Num: 178 Reward: 657.972988930698\n",
            "Total Timesteps: 179000 Episode Num: 179 Reward: 500.6868004614047\n",
            "Total Timesteps: 180000 Episode Num: 180 Reward: 619.1322608385858\n",
            "Total Timesteps: 181000 Episode Num: 181 Reward: 600.4137531597066\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 578.8076927630057\n",
            "---------------------------------------\n",
            "Total Timesteps: 182000 Episode Num: 182 Reward: 620.7219189666177\n",
            "Total Timesteps: 183000 Episode Num: 183 Reward: 628.5355999283445\n",
            "Total Timesteps: 184000 Episode Num: 184 Reward: 607.2920374575193\n",
            "Total Timesteps: 185000 Episode Num: 185 Reward: 599.6552127142611\n",
            "Total Timesteps: 186000 Episode Num: 186 Reward: 621.6740308415639\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 606.0704052897698\n",
            "---------------------------------------\n",
            "Total Timesteps: 187000 Episode Num: 187 Reward: 595.2501067689245\n",
            "Total Timesteps: 188000 Episode Num: 188 Reward: 627.0285356837941\n",
            "Total Timesteps: 189000 Episode Num: 189 Reward: 591.9022485926872\n",
            "Total Timesteps: 190000 Episode Num: 190 Reward: 621.310371055106\n",
            "Total Timesteps: 191000 Episode Num: 191 Reward: 602.3635105830532\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 645.4283278667278\n",
            "---------------------------------------\n",
            "Total Timesteps: 192000 Episode Num: 192 Reward: 640.0893320975897\n",
            "Total Timesteps: 193000 Episode Num: 193 Reward: 547.6039097381642\n",
            "Total Timesteps: 194000 Episode Num: 194 Reward: 677.8700874046287\n",
            "Total Timesteps: 195000 Episode Num: 195 Reward: 621.971296261919\n",
            "Total Timesteps: 196000 Episode Num: 196 Reward: 633.7120083277957\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 508.03465896402656\n",
            "---------------------------------------\n",
            "Total Timesteps: 197000 Episode Num: 197 Reward: 465.57959363413755\n",
            "Total Timesteps: 198000 Episode Num: 198 Reward: 461.6133531367486\n",
            "Total Timesteps: 199000 Episode Num: 199 Reward: 710.7962530105868\n",
            "Total Timesteps: 200000 Episode Num: 200 Reward: 764.1286741422089\n",
            "Total Timesteps: 201000 Episode Num: 201 Reward: 518.235666890199\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 706.51225105679\n",
            "---------------------------------------\n",
            "Total Timesteps: 202000 Episode Num: 202 Reward: 680.2584171760043\n",
            "Total Timesteps: 203000 Episode Num: 203 Reward: 635.5078699335229\n",
            "Total Timesteps: 204000 Episode Num: 204 Reward: 725.7449945176307\n",
            "Total Timesteps: 205000 Episode Num: 205 Reward: 733.2899584002207\n",
            "Total Timesteps: 206000 Episode Num: 206 Reward: 714.7229715039487\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 705.0520676420513\n",
            "---------------------------------------\n",
            "Total Timesteps: 207000 Episode Num: 207 Reward: 723.3748458024484\n",
            "Total Timesteps: 208000 Episode Num: 208 Reward: 692.0973154995557\n",
            "Total Timesteps: 209000 Episode Num: 209 Reward: 723.3338929178612\n",
            "Total Timesteps: 210000 Episode Num: 210 Reward: 835.4816147043749\n",
            "Total Timesteps: 211000 Episode Num: 211 Reward: 759.9559214765641\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 696.9570882917824\n",
            "---------------------------------------\n",
            "Total Timesteps: 212000 Episode Num: 212 Reward: 696.7212329727131\n",
            "Total Timesteps: 213000 Episode Num: 213 Reward: 700.0821781332507\n",
            "Total Timesteps: 214000 Episode Num: 214 Reward: 635.7089434609164\n",
            "Total Timesteps: 215000 Episode Num: 215 Reward: 634.6278394367906\n",
            "Total Timesteps: 216000 Episode Num: 216 Reward: 635.6333697895054\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 528.5704673915441\n",
            "---------------------------------------\n",
            "Total Timesteps: 217000 Episode Num: 217 Reward: 490.34682721730195\n",
            "Total Timesteps: 218000 Episode Num: 218 Reward: 563.8973140854349\n",
            "Total Timesteps: 219000 Episode Num: 219 Reward: 613.7052309467855\n",
            "Total Timesteps: 220000 Episode Num: 220 Reward: 603.8548780228198\n",
            "Total Timesteps: 221000 Episode Num: 221 Reward: 716.5336281688483\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 548.7853100936863\n",
            "---------------------------------------\n",
            "Total Timesteps: 222000 Episode Num: 222 Reward: 574.5415505641522\n",
            "Total Timesteps: 223000 Episode Num: 223 Reward: 679.0035846467472\n",
            "Total Timesteps: 224000 Episode Num: 224 Reward: 679.2002208497247\n",
            "Total Timesteps: 225000 Episode Num: 225 Reward: 710.4041989754659\n",
            "Total Timesteps: 226000 Episode Num: 226 Reward: 699.3805294177935\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 694.1123207566582\n",
            "---------------------------------------\n",
            "Total Timesteps: 227000 Episode Num: 227 Reward: 705.996154340874\n",
            "Total Timesteps: 228000 Episode Num: 228 Reward: 648.0117091745738\n",
            "Total Timesteps: 229000 Episode Num: 229 Reward: 622.1829294098146\n",
            "Total Timesteps: 230000 Episode Num: 230 Reward: 668.5212889814926\n",
            "Total Timesteps: 231000 Episode Num: 231 Reward: 595.489982176686\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 690.055025533695\n",
            "---------------------------------------\n",
            "Total Timesteps: 232000 Episode Num: 232 Reward: 713.1189165968144\n",
            "Total Timesteps: 233000 Episode Num: 233 Reward: 680.7696427990295\n",
            "Total Timesteps: 234000 Episode Num: 234 Reward: 639.032162481916\n",
            "Total Timesteps: 235000 Episode Num: 235 Reward: 680.5236631858202\n",
            "Total Timesteps: 236000 Episode Num: 236 Reward: 780.8340183069043\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 611.0920230146744\n",
            "---------------------------------------\n",
            "Total Timesteps: 237000 Episode Num: 237 Reward: 759.9804361971162\n",
            "Total Timesteps: 238000 Episode Num: 238 Reward: 717.1634761204784\n",
            "Total Timesteps: 239000 Episode Num: 239 Reward: 839.5226689938683\n",
            "Total Timesteps: 240000 Episode Num: 240 Reward: 835.2132902895871\n",
            "Total Timesteps: 241000 Episode Num: 241 Reward: 956.9177977306975\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 923.9232222668392\n",
            "---------------------------------------\n",
            "Total Timesteps: 242000 Episode Num: 242 Reward: 890.7869784307455\n",
            "Total Timesteps: 243000 Episode Num: 243 Reward: 876.885276335294\n",
            "Total Timesteps: 244000 Episode Num: 244 Reward: 856.0573458748436\n",
            "Total Timesteps: 245000 Episode Num: 245 Reward: 850.1068176590366\n",
            "Total Timesteps: 246000 Episode Num: 246 Reward: 939.2750403470385\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 888.9417585460326\n",
            "---------------------------------------\n",
            "Total Timesteps: 247000 Episode Num: 247 Reward: 815.0243075446922\n",
            "Total Timesteps: 248000 Episode Num: 248 Reward: 909.9764859531601\n",
            "Total Timesteps: 249000 Episode Num: 249 Reward: 874.5453613476952\n",
            "Total Timesteps: 250000 Episode Num: 250 Reward: 856.7961994873035\n",
            "Total Timesteps: 251000 Episode Num: 251 Reward: 838.0870895099358\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 668.6798223440289\n",
            "---------------------------------------\n",
            "Total Timesteps: 252000 Episode Num: 252 Reward: 617.2186467020723\n",
            "Total Timesteps: 253000 Episode Num: 253 Reward: 694.8189070421674\n",
            "Total Timesteps: 254000 Episode Num: 254 Reward: 589.95708899755\n",
            "Total Timesteps: 255000 Episode Num: 255 Reward: 770.3317250764353\n",
            "Total Timesteps: 256000 Episode Num: 256 Reward: 782.4388514175095\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 624.6114564074267\n",
            "---------------------------------------\n",
            "Total Timesteps: 257000 Episode Num: 257 Reward: 615.5356902617411\n",
            "Total Timesteps: 258000 Episode Num: 258 Reward: 732.7363757467845\n",
            "Total Timesteps: 259000 Episode Num: 259 Reward: 784.4317775620232\n",
            "Total Timesteps: 260000 Episode Num: 260 Reward: 753.4514722635025\n",
            "Total Timesteps: 261000 Episode Num: 261 Reward: 780.5647382214294\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 728.2785298969509\n",
            "---------------------------------------\n",
            "Total Timesteps: 262000 Episode Num: 262 Reward: 711.4761310439711\n",
            "Total Timesteps: 263000 Episode Num: 263 Reward: 699.0292822798003\n",
            "Total Timesteps: 264000 Episode Num: 264 Reward: 794.3287983286932\n",
            "Total Timesteps: 265000 Episode Num: 265 Reward: 825.7492157620328\n",
            "Total Timesteps: 266000 Episode Num: 266 Reward: 777.79226288947\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 802.9517980624374\n",
            "---------------------------------------\n",
            "Total Timesteps: 267000 Episode Num: 267 Reward: 794.1581565042919\n",
            "Total Timesteps: 268000 Episode Num: 268 Reward: 796.3657461691867\n",
            "Total Timesteps: 269000 Episode Num: 269 Reward: 858.3307034919541\n",
            "Total Timesteps: 270000 Episode Num: 270 Reward: 767.6134791772934\n",
            "Total Timesteps: 271000 Episode Num: 271 Reward: 777.4687705115006\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 802.0258637348629\n",
            "---------------------------------------\n",
            "Total Timesteps: 272000 Episode Num: 272 Reward: 770.8848059721508\n",
            "Total Timesteps: 273000 Episode Num: 273 Reward: 800.4992054664707\n",
            "Total Timesteps: 274000 Episode Num: 274 Reward: 793.7934334529234\n",
            "Total Timesteps: 275000 Episode Num: 275 Reward: 814.1153042996459\n",
            "Total Timesteps: 276000 Episode Num: 276 Reward: 844.942006650668\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 801.901814305579\n",
            "---------------------------------------\n",
            "Total Timesteps: 277000 Episode Num: 277 Reward: 742.5208016901684\n",
            "Total Timesteps: 278000 Episode Num: 278 Reward: 859.5484319283792\n",
            "Total Timesteps: 279000 Episode Num: 279 Reward: 888.9572031245427\n",
            "Total Timesteps: 280000 Episode Num: 280 Reward: 881.4183715723192\n",
            "Total Timesteps: 281000 Episode Num: 281 Reward: 824.736670009431\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 799.6255708427867\n",
            "---------------------------------------\n",
            "Total Timesteps: 282000 Episode Num: 282 Reward: 784.4190713710833\n",
            "Total Timesteps: 283000 Episode Num: 283 Reward: 814.2342876220589\n",
            "Total Timesteps: 284000 Episode Num: 284 Reward: 769.0460020147723\n",
            "Total Timesteps: 285000 Episode Num: 285 Reward: 738.2738784310767\n",
            "Total Timesteps: 286000 Episode Num: 286 Reward: 790.9621215511758\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 847.0787675468185\n",
            "---------------------------------------\n",
            "Total Timesteps: 287000 Episode Num: 287 Reward: 828.0456594943679\n",
            "Total Timesteps: 288000 Episode Num: 288 Reward: 896.428390929743\n",
            "Total Timesteps: 289000 Episode Num: 289 Reward: 921.6508045573247\n",
            "Total Timesteps: 290000 Episode Num: 290 Reward: 878.581027078539\n",
            "Total Timesteps: 291000 Episode Num: 291 Reward: 858.145616353423\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 947.2783666507119\n",
            "---------------------------------------\n",
            "Total Timesteps: 292000 Episode Num: 292 Reward: 908.2584816573706\n",
            "Total Timesteps: 293000 Episode Num: 293 Reward: 872.5787237107786\n",
            "Total Timesteps: 294000 Episode Num: 294 Reward: 893.1862788711875\n",
            "Total Timesteps: 295000 Episode Num: 295 Reward: 900.8345432851858\n",
            "Total Timesteps: 296000 Episode Num: 296 Reward: 847.4359616330075\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 886.8158584779527\n",
            "---------------------------------------\n",
            "Total Timesteps: 297000 Episode Num: 297 Reward: 870.9151218065746\n",
            "Total Timesteps: 298000 Episode Num: 298 Reward: 917.2735366574545\n",
            "Total Timesteps: 299000 Episode Num: 299 Reward: 805.5031942862607\n",
            "Total Timesteps: 300000 Episode Num: 300 Reward: 1013.5831785144393\n",
            "Total Timesteps: 301000 Episode Num: 301 Reward: 917.8368982954596\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 929.2088709522452\n",
            "---------------------------------------\n",
            "Total Timesteps: 302000 Episode Num: 302 Reward: 914.2413645177153\n",
            "Total Timesteps: 303000 Episode Num: 303 Reward: 933.6661488069244\n",
            "Total Timesteps: 304000 Episode Num: 304 Reward: 956.2360670280893\n",
            "Total Timesteps: 305000 Episode Num: 305 Reward: 953.8971214884521\n",
            "Total Timesteps: 306000 Episode Num: 306 Reward: 849.2043789237136\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 925.281687258532\n",
            "---------------------------------------\n",
            "Total Timesteps: 307000 Episode Num: 307 Reward: 911.1851729386657\n",
            "Total Timesteps: 308000 Episode Num: 308 Reward: 929.2819958727791\n",
            "Total Timesteps: 309000 Episode Num: 309 Reward: 949.6149934248848\n",
            "Total Timesteps: 310000 Episode Num: 310 Reward: 957.358866111873\n",
            "Total Timesteps: 311000 Episode Num: 311 Reward: 968.4690526045188\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 983.6633332782824\n",
            "---------------------------------------\n",
            "Total Timesteps: 312000 Episode Num: 312 Reward: 975.0004140695676\n",
            "Total Timesteps: 313000 Episode Num: 313 Reward: 908.6068745513224\n",
            "Total Timesteps: 314000 Episode Num: 314 Reward: 849.5838050089164\n",
            "Total Timesteps: 315000 Episode Num: 315 Reward: 930.5170358192994\n",
            "Total Timesteps: 316000 Episode Num: 316 Reward: 914.2883947093811\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 993.1616128015252\n",
            "---------------------------------------\n",
            "Total Timesteps: 317000 Episode Num: 317 Reward: 972.5965086737264\n",
            "Total Timesteps: 318000 Episode Num: 318 Reward: 958.0291382841993\n",
            "Total Timesteps: 319000 Episode Num: 319 Reward: 964.8305949370456\n",
            "Total Timesteps: 320000 Episode Num: 320 Reward: 985.5781209307804\n",
            "Total Timesteps: 321000 Episode Num: 321 Reward: 977.5125196916174\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 926.0572849944344\n",
            "---------------------------------------\n",
            "Total Timesteps: 322000 Episode Num: 322 Reward: 939.5132541594527\n",
            "Total Timesteps: 323000 Episode Num: 323 Reward: 904.4694824777464\n",
            "Total Timesteps: 324000 Episode Num: 324 Reward: 949.5426548815502\n",
            "Total Timesteps: 325000 Episode Num: 325 Reward: 888.0962302027413\n",
            "Total Timesteps: 326000 Episode Num: 326 Reward: 801.2422751920631\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 885.7507251138279\n",
            "---------------------------------------\n",
            "Total Timesteps: 327000 Episode Num: 327 Reward: 864.6316874489277\n",
            "Total Timesteps: 328000 Episode Num: 328 Reward: 840.1957114899702\n",
            "Total Timesteps: 329000 Episode Num: 329 Reward: 892.7083134320236\n",
            "Total Timesteps: 330000 Episode Num: 330 Reward: 922.9415091802416\n",
            "Total Timesteps: 331000 Episode Num: 331 Reward: 903.3757888108429\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 948.8259470960331\n",
            "---------------------------------------\n",
            "Total Timesteps: 332000 Episode Num: 332 Reward: 943.0332819711014\n",
            "Total Timesteps: 333000 Episode Num: 333 Reward: 899.440350707499\n",
            "Total Timesteps: 334000 Episode Num: 334 Reward: 945.9045755479102\n",
            "Total Timesteps: 335000 Episode Num: 335 Reward: 973.2791794599364\n",
            "Total Timesteps: 336000 Episode Num: 336 Reward: 968.3013217236804\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 992.0199128935035\n",
            "---------------------------------------\n",
            "Total Timesteps: 337000 Episode Num: 337 Reward: 991.9371280900224\n",
            "Total Timesteps: 338000 Episode Num: 338 Reward: 951.057916389251\n",
            "Total Timesteps: 339000 Episode Num: 339 Reward: 947.159208257843\n",
            "Total Timesteps: 340000 Episode Num: 340 Reward: 702.9097962988523\n",
            "Total Timesteps: 341000 Episode Num: 341 Reward: 964.040539079312\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 982.5124722238285\n",
            "---------------------------------------\n",
            "Total Timesteps: 342000 Episode Num: 342 Reward: 965.9478297387168\n",
            "Total Timesteps: 343000 Episode Num: 343 Reward: 963.8094999066159\n",
            "Total Timesteps: 344000 Episode Num: 344 Reward: 576.1914345397663\n",
            "Total Timesteps: 345000 Episode Num: 345 Reward: 1005.3366696820888\n",
            "Total Timesteps: 346000 Episode Num: 346 Reward: 1002.6042075951918\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1006.3792793791126\n",
            "---------------------------------------\n",
            "Total Timesteps: 347000 Episode Num: 347 Reward: 989.80392072901\n",
            "Total Timesteps: 348000 Episode Num: 348 Reward: 976.3915761437363\n",
            "Total Timesteps: 349000 Episode Num: 349 Reward: 927.9184906114901\n",
            "Total Timesteps: 350000 Episode Num: 350 Reward: 1002.8871066529678\n",
            "Total Timesteps: 351000 Episode Num: 351 Reward: 1042.4100124976762\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1056.467271132712\n",
            "---------------------------------------\n",
            "Total Timesteps: 352000 Episode Num: 352 Reward: 1038.5169514407828\n",
            "Total Timesteps: 353000 Episode Num: 353 Reward: 987.0782521253674\n",
            "Total Timesteps: 354000 Episode Num: 354 Reward: 941.2215199716716\n",
            "Total Timesteps: 355000 Episode Num: 355 Reward: 1069.511058865385\n",
            "Total Timesteps: 356000 Episode Num: 356 Reward: 1038.5482003473078\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1030.415413580342\n",
            "---------------------------------------\n",
            "Total Timesteps: 357000 Episode Num: 357 Reward: 1009.530558185679\n",
            "Total Timesteps: 358000 Episode Num: 358 Reward: 1052.2853706528715\n",
            "Total Timesteps: 359000 Episode Num: 359 Reward: 991.5878064189526\n",
            "Total Timesteps: 360000 Episode Num: 360 Reward: 978.5219424952672\n",
            "Total Timesteps: 361000 Episode Num: 361 Reward: 1030.1423019080953\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1048.3736629858533\n",
            "---------------------------------------\n",
            "Total Timesteps: 362000 Episode Num: 362 Reward: 1000.7883062125676\n",
            "Total Timesteps: 363000 Episode Num: 363 Reward: 960.579445804347\n",
            "Total Timesteps: 364000 Episode Num: 364 Reward: 938.5014343766866\n",
            "Total Timesteps: 365000 Episode Num: 365 Reward: 973.9489060385413\n",
            "Total Timesteps: 366000 Episode Num: 366 Reward: 978.1875928985633\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1020.7956219385218\n",
            "---------------------------------------\n",
            "Total Timesteps: 367000 Episode Num: 367 Reward: 974.3062430172258\n",
            "Total Timesteps: 368000 Episode Num: 368 Reward: 938.5638018970508\n",
            "Total Timesteps: 369000 Episode Num: 369 Reward: 967.6775681905918\n",
            "Total Timesteps: 370000 Episode Num: 370 Reward: 1002.4662247061717\n",
            "Total Timesteps: 371000 Episode Num: 371 Reward: 1059.7195058504424\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1057.4790933820439\n",
            "---------------------------------------\n",
            "Total Timesteps: 372000 Episode Num: 372 Reward: 1028.0546099663845\n",
            "Total Timesteps: 373000 Episode Num: 373 Reward: 1038.0406670562309\n",
            "Total Timesteps: 374000 Episode Num: 374 Reward: 1014.113207929503\n",
            "Total Timesteps: 375000 Episode Num: 375 Reward: 1012.5865556075108\n",
            "Total Timesteps: 376000 Episode Num: 376 Reward: 961.7308691198057\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1052.8639819065343\n",
            "---------------------------------------\n",
            "Total Timesteps: 377000 Episode Num: 377 Reward: 1047.3964482079334\n",
            "Total Timesteps: 378000 Episode Num: 378 Reward: 952.3514950905563\n",
            "Total Timesteps: 379000 Episode Num: 379 Reward: 872.4728145684252\n",
            "Total Timesteps: 380000 Episode Num: 380 Reward: 983.833284383079\n",
            "Total Timesteps: 381000 Episode Num: 381 Reward: 1034.9966071985539\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1081.4045850704301\n",
            "---------------------------------------\n",
            "Total Timesteps: 382000 Episode Num: 382 Reward: 1060.9483690943339\n",
            "Total Timesteps: 383000 Episode Num: 383 Reward: 989.7482172161334\n",
            "Total Timesteps: 384000 Episode Num: 384 Reward: 975.5391822641923\n",
            "Total Timesteps: 385000 Episode Num: 385 Reward: 1004.1607604964047\n",
            "Total Timesteps: 386000 Episode Num: 386 Reward: 977.1658413421579\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1085.021116336358\n",
            "---------------------------------------\n",
            "Total Timesteps: 387000 Episode Num: 387 Reward: 1038.5666862014602\n",
            "Total Timesteps: 388000 Episode Num: 388 Reward: 975.6763806817702\n",
            "Total Timesteps: 389000 Episode Num: 389 Reward: 1037.256992740641\n",
            "Total Timesteps: 390000 Episode Num: 390 Reward: 1014.6620165526884\n",
            "Total Timesteps: 391000 Episode Num: 391 Reward: 1057.0678520630352\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1093.5702630990688\n",
            "---------------------------------------\n",
            "Total Timesteps: 392000 Episode Num: 392 Reward: 1053.6785936189742\n",
            "Total Timesteps: 393000 Episode Num: 393 Reward: 1059.954963105087\n",
            "Total Timesteps: 394000 Episode Num: 394 Reward: 972.786967284793\n",
            "Total Timesteps: 395000 Episode Num: 395 Reward: 1085.1104601973336\n",
            "Total Timesteps: 396000 Episode Num: 396 Reward: 1029.1128683544703\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1089.3995299251583\n",
            "---------------------------------------\n",
            "Total Timesteps: 397000 Episode Num: 397 Reward: 1076.4638377494082\n",
            "Total Timesteps: 398000 Episode Num: 398 Reward: 1055.4019336264398\n",
            "Total Timesteps: 399000 Episode Num: 399 Reward: 1017.9238326608272\n",
            "Total Timesteps: 400000 Episode Num: 400 Reward: 1029.6428872420495\n",
            "Total Timesteps: 401000 Episode Num: 401 Reward: 1027.7228313911073\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1066.8397913804442\n",
            "---------------------------------------\n",
            "Total Timesteps: 402000 Episode Num: 402 Reward: 1025.1482901540064\n",
            "Total Timesteps: 403000 Episode Num: 403 Reward: 1030.341568487667\n",
            "Total Timesteps: 404000 Episode Num: 404 Reward: 998.4218853671864\n",
            "Total Timesteps: 405000 Episode Num: 405 Reward: 1037.7167711537693\n",
            "Total Timesteps: 406000 Episode Num: 406 Reward: 1044.0645695217474\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1009.6789024038941\n",
            "---------------------------------------\n",
            "Total Timesteps: 407000 Episode Num: 407 Reward: 956.9973536563564\n",
            "Total Timesteps: 408000 Episode Num: 408 Reward: 1020.4247934352636\n",
            "Total Timesteps: 409000 Episode Num: 409 Reward: 1097.1955226779492\n",
            "Total Timesteps: 410000 Episode Num: 410 Reward: 1115.2381631472317\n",
            "Total Timesteps: 411000 Episode Num: 411 Reward: 1086.9612399601053\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1122.7916977238492\n",
            "---------------------------------------\n",
            "Total Timesteps: 412000 Episode Num: 412 Reward: 1065.5593838073469\n",
            "Total Timesteps: 413000 Episode Num: 413 Reward: 1084.115917521209\n",
            "Total Timesteps: 414000 Episode Num: 414 Reward: 1078.6574424087225\n",
            "Total Timesteps: 415000 Episode Num: 415 Reward: 1076.5585672153275\n",
            "Total Timesteps: 416000 Episode Num: 416 Reward: 1062.5147413972963\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1118.9665721964907\n",
            "---------------------------------------\n",
            "Total Timesteps: 417000 Episode Num: 417 Reward: 1109.3811937826633\n",
            "Total Timesteps: 418000 Episode Num: 418 Reward: 1120.8131965264831\n",
            "Total Timesteps: 419000 Episode Num: 419 Reward: 1102.2629783567581\n",
            "Total Timesteps: 420000 Episode Num: 420 Reward: 1145.568915653179\n",
            "Total Timesteps: 421000 Episode Num: 421 Reward: 1156.0555738031933\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1171.779474904149\n",
            "---------------------------------------\n",
            "Total Timesteps: 422000 Episode Num: 422 Reward: 1130.9264253732742\n",
            "Total Timesteps: 423000 Episode Num: 423 Reward: 1147.014991846169\n",
            "Total Timesteps: 424000 Episode Num: 424 Reward: 1123.9664645156265\n",
            "Total Timesteps: 425000 Episode Num: 425 Reward: 1132.2877303512025\n",
            "Total Timesteps: 426000 Episode Num: 426 Reward: 1160.2068494343682\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1183.4227704721475\n",
            "---------------------------------------\n",
            "Total Timesteps: 427000 Episode Num: 427 Reward: 1155.7340071542603\n",
            "Total Timesteps: 428000 Episode Num: 428 Reward: 1162.684688970295\n",
            "Total Timesteps: 429000 Episode Num: 429 Reward: 1123.1325973233031\n",
            "Total Timesteps: 430000 Episode Num: 430 Reward: 1153.7304313018492\n",
            "Total Timesteps: 431000 Episode Num: 431 Reward: 1148.1302611394958\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1174.940213976321\n",
            "---------------------------------------\n",
            "Total Timesteps: 432000 Episode Num: 432 Reward: 1159.0512152620474\n",
            "Total Timesteps: 433000 Episode Num: 433 Reward: 1103.5448236848097\n",
            "Total Timesteps: 434000 Episode Num: 434 Reward: 1131.074539800604\n",
            "Total Timesteps: 435000 Episode Num: 435 Reward: 1146.6987977644949\n",
            "Total Timesteps: 436000 Episode Num: 436 Reward: 1160.0940555025427\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1166.04400572663\n",
            "---------------------------------------\n",
            "Total Timesteps: 437000 Episode Num: 437 Reward: 1158.4756911263191\n",
            "Total Timesteps: 438000 Episode Num: 438 Reward: 1175.764071383432\n",
            "Total Timesteps: 439000 Episode Num: 439 Reward: 1211.5868397634824\n",
            "Total Timesteps: 440000 Episode Num: 440 Reward: 1144.0826156366554\n",
            "Total Timesteps: 441000 Episode Num: 441 Reward: 1165.6549187600633\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1124.5267110242082\n",
            "---------------------------------------\n",
            "Total Timesteps: 442000 Episode Num: 442 Reward: 1135.9075197312316\n",
            "Total Timesteps: 443000 Episode Num: 443 Reward: 1151.7444973207346\n",
            "Total Timesteps: 444000 Episode Num: 444 Reward: 1149.6668151525157\n",
            "Total Timesteps: 445000 Episode Num: 445 Reward: 1101.8747332012172\n",
            "Total Timesteps: 446000 Episode Num: 446 Reward: 1117.0473183948263\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1205.14510002751\n",
            "---------------------------------------\n",
            "Total Timesteps: 447000 Episode Num: 447 Reward: 1179.4115661775638\n",
            "Total Timesteps: 448000 Episode Num: 448 Reward: 1191.8450209898954\n",
            "Total Timesteps: 449000 Episode Num: 449 Reward: 1134.924081758828\n",
            "Total Timesteps: 450000 Episode Num: 450 Reward: 1132.3870308598723\n",
            "Total Timesteps: 451000 Episode Num: 451 Reward: 1184.4149062689557\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1240.1210956873533\n",
            "---------------------------------------\n",
            "Total Timesteps: 452000 Episode Num: 452 Reward: 1197.0544908023612\n",
            "Total Timesteps: 453000 Episode Num: 453 Reward: 1168.611316217856\n",
            "Total Timesteps: 454000 Episode Num: 454 Reward: 1155.364103439463\n",
            "Total Timesteps: 455000 Episode Num: 455 Reward: 1205.3071376506502\n",
            "Total Timesteps: 456000 Episode Num: 456 Reward: 1057.6345416264971\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1172.2224693835142\n",
            "---------------------------------------\n",
            "Total Timesteps: 457000 Episode Num: 457 Reward: 1157.8403213786341\n",
            "Total Timesteps: 458000 Episode Num: 458 Reward: 1227.8659195773828\n",
            "Total Timesteps: 459000 Episode Num: 459 Reward: 1229.5181237462637\n",
            "Total Timesteps: 460000 Episode Num: 460 Reward: 1217.8438905711962\n",
            "Total Timesteps: 461000 Episode Num: 461 Reward: 1192.894160161297\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1276.4345689972158\n",
            "---------------------------------------\n",
            "Total Timesteps: 462000 Episode Num: 462 Reward: 1242.367653786686\n",
            "Total Timesteps: 463000 Episode Num: 463 Reward: 1209.119071447129\n",
            "Total Timesteps: 464000 Episode Num: 464 Reward: 1184.6542855370622\n",
            "Total Timesteps: 465000 Episode Num: 465 Reward: 1252.939945167256\n",
            "Total Timesteps: 466000 Episode Num: 466 Reward: 1143.7421186788088\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1227.842627155544\n",
            "---------------------------------------\n",
            "Total Timesteps: 467000 Episode Num: 467 Reward: 1169.1483951130879\n",
            "Total Timesteps: 468000 Episode Num: 468 Reward: 1188.801849363458\n",
            "Total Timesteps: 469000 Episode Num: 469 Reward: 1194.786431448621\n",
            "Total Timesteps: 470000 Episode Num: 470 Reward: 1209.5117698095667\n",
            "Total Timesteps: 471000 Episode Num: 471 Reward: 1223.5574954071503\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1225.023572588543\n",
            "---------------------------------------\n",
            "Total Timesteps: 472000 Episode Num: 472 Reward: 1203.91605506847\n",
            "Total Timesteps: 473000 Episode Num: 473 Reward: 1189.6627908535868\n",
            "Total Timesteps: 474000 Episode Num: 474 Reward: 1204.4950838852963\n",
            "Total Timesteps: 475000 Episode Num: 475 Reward: 1210.459731357989\n",
            "Total Timesteps: 476000 Episode Num: 476 Reward: 1214.3637021966872\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1313.2951762717232\n",
            "---------------------------------------\n",
            "Total Timesteps: 477000 Episode Num: 477 Reward: 1266.2638332361748\n",
            "Total Timesteps: 478000 Episode Num: 478 Reward: 1212.8504874547302\n",
            "Total Timesteps: 479000 Episode Num: 479 Reward: 1160.742104229569\n",
            "Total Timesteps: 480000 Episode Num: 480 Reward: 1175.324754099473\n",
            "Total Timesteps: 481000 Episode Num: 481 Reward: 1156.5373226347338\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1281.6191212883616\n",
            "---------------------------------------\n",
            "Total Timesteps: 482000 Episode Num: 482 Reward: 1236.3191099547846\n",
            "Total Timesteps: 483000 Episode Num: 483 Reward: 1210.0355186257727\n",
            "Total Timesteps: 484000 Episode Num: 484 Reward: 1203.4560880146666\n",
            "Total Timesteps: 485000 Episode Num: 485 Reward: 1255.7353587735258\n",
            "Total Timesteps: 486000 Episode Num: 486 Reward: 1211.1467504394755\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1257.0869855750461\n",
            "---------------------------------------\n",
            "Total Timesteps: 487000 Episode Num: 487 Reward: 1197.7042414355153\n",
            "Total Timesteps: 488000 Episode Num: 488 Reward: 1204.5597148378477\n",
            "Total Timesteps: 489000 Episode Num: 489 Reward: 1171.8542595351391\n",
            "Total Timesteps: 490000 Episode Num: 490 Reward: 1204.1844484827004\n",
            "Total Timesteps: 491000 Episode Num: 491 Reward: 1224.6617043824062\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1238.8103536998517\n",
            "---------------------------------------\n",
            "Total Timesteps: 492000 Episode Num: 492 Reward: 1213.1308152852932\n",
            "Total Timesteps: 493000 Episode Num: 493 Reward: 1230.1315711481395\n",
            "Total Timesteps: 494000 Episode Num: 494 Reward: 1201.0989090320645\n",
            "Total Timesteps: 495000 Episode Num: 495 Reward: 1213.9097153010528\n",
            "Total Timesteps: 496000 Episode Num: 496 Reward: 1061.58845197238\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1235.5578565795765\n",
            "---------------------------------------\n",
            "Total Timesteps: 497000 Episode Num: 497 Reward: 1205.924534425807\n",
            "Total Timesteps: 498000 Episode Num: 498 Reward: 1200.0403520933555\n",
            "Total Timesteps: 499000 Episode Num: 499 Reward: 1168.142926924108\n",
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1252.6311437949414\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmH6QW7CNHcI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "c8f00e07-0410-499c-cced-80001b369538"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_actions):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_actions = max_actions\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_actions * torch.tanh(self.layer_3(x))\n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x = F.relu(self.layer_1(xu))\n",
        "    x = f.relu(self.layer_2(x))\n",
        "    x = self.layer_3(x)\n",
        "    return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TD3(object):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iteration, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iteration):\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "      next_action = self.actor_target(next_state)\n",
        "\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      target_Q = reward + ((1-done) * discount * target_Q).detach()\n",
        "\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward=0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print(f'Average Reward for the Evaluation step : {avg_reward}')\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"HalfCheetahBulletEnv-v0\"\n",
        "# env_name = \"Walker2DBulletEnv-v0\"\n",
        "# env_name = \"AntBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = '%s_%s_%s' % ('TD3', env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print(f'Setting : {file_name}')\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "\n",
        "save_env_vid = True\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force=True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Setting : TD3_HalfCheetahBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward for the Evaluation step : 1255.5034484303055\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTskiRbFmP4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}